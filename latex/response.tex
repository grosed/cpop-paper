\title{cpop: Detecting changes in piecewise-linear signals -- response to Reviewers}
\author{Paul Fearnhead and Daniel Grose}
\date{\today}



\documentclass[12pt]{article}

\usepackage[dvipsnames]{xcolor}

\begin{document}
\maketitle

Below we reproduce the comments on the original version of our paper ``cpop: Detecting changes in piecewise-linear signals", together with a point-by-point responses and summary of the changes we have made to the paper and the package. The original comments are in italics.

\vspace*{1ex}

\noindent
{\bf Response to Reviewer D}

\begin{itemize}
\item {\em   Since a piecewise-linear function is a special case of a piecewise-polynomial function, it seems relevant to point out research in detecting changes in piecewise-polynomial trend functions and related software.}

We have added a brief paragraph discussing this link towards the end of the introduction. 

\item {\em   For the function $F_l(\alpha)$ defined on page 7, it is necessary to remark on the case when $l = 1$. The current definition seems incorrect.}

We define the recursion (7) so that it is valid for $l=1$. The initial condition depends on $F_0(\alpha)$, and this is specified after (7) as $F_0(\alpha)=-\beta$. Thus if we consider (7) for $l=1$ with $F_0(\alpha)=-\beta)$ we have
\[
F_1(\alpha)=\min_{\alpha'} \left[-\beta +  \mathcal{C}_{0,1}(\alpha,\alpha)  +\beta \right]=  \min_{\alpha'}   ~~ \mathcal{C}_{0,1}(\alpha,\alpha). 
\]
As we can have only one segment in this case, this is the correct definition of $F_1(\alpha)$ -- i.e. it is equal to (2) with $K=0$ and conditional on $\alpha_1=\alpha$ (using the definition of $\mathcal{C}_{0,1}(\alpha,\alpha)$).

\item {\em   In Section 4.4, it is mentioned that ‘cpop is slower when a minimum segment length is imposed.’ This claim seems counterintuitive since a large value of the minimum segment length would reduce the computation cost of dynamic programming.}

This is an interesting point. If we were to set a maximum segment length, then this would reduce the computation of dynamic programming, as we can discard all candidate changes that are not consistent with the maximum segment length. But this is possible because these candidate changes can never be the value of the most recent change at later time points (if at time $t$ they produce a segment which is too large, then they would produce a segment that is too large for times $s>t$). This is not the case for a minimum segment length -- we cannot remove candidate changes as they could be relevant at later time points. Thus there is not the same reduction in computational cost.

By comparison they can make the algorithm slower because the PELT pruning ideas that are used to speed up cpop cannot be used when a minimum segment length is imposed. We do suggest an approximate pruning, but this appears to be marginally less effective. We comment on this in the paper, for example at the end of Section 2 and give the empirical increase in computational cost in Section 4.4.

\end{itemize}

Thank you for pointing out the minor typos. These have all been corrected.

\vspace*{1em}

\noindent
{\bf Response to Reviewer E}
\begin{itemize}
\item {\em   A reference is missing: [1]. How does the method compare to the approach in such paper? It would be interesting to replace trend filtering with the method from [1] in Figure 1.}

The method in [1] does not solve the change-in-slope problem, as it does not impose continuity at the changes. As such we do not think it is as relevant as trend-filtering, which does solve the same problem as CPOP, and have kept the trend filtering results. However, we think this is an interesting alternative and have expanded Figure 1 to include an example of the output of this method -- demonstrating the different model that is being fit. We have also added a paragraph to the introduction which cites this work.

\item {\em   Another reference missing is the R package changepoints}

We have added a reference to this package. 

\item {\em   In Section 3.2, why is the default choice of sd set to 
\begin{verbatim}
(mean(diff(diff(y))^ 2)/6)
\end{verbatim}
In particular, where does the factor 6 come from?}

If we let $\Delta$ be the the difference operator then the $(t-2)$th entry of $\Delta(\Delta y)$ is
\[
(y_t-y_{t-1})-(y_{t-1}-y_{t-2})=y_t-2y_{t-1}+y_{t-2}.
\]
If $y$ is linear mean plus noise, then the mean of this will be 0, so the variance is the mean of its square. This variance is $\sigma^2+4\sigma^2+\sigma^2=6\sigma^2$, where $\sigma^2$ is the noise variance and we have assumed independence. Hence we need to re-scale by dividing by 6 to get an estimator of the noise variance. We have added a brief comment on this in Section 3.2.

\item {\em   Also in Section 3.2, can the authors elaborate on the output of fitted values? For instance, what does the column RSS represent?}

The RSS values are the (weighted) residual sum of squares of the fitted mean within the corresponding segment. This is mentioned just before the example in Section 3.2.

\item {\em  In Figure 6, is the plot shown corresponding to regular data? how does the computational time behave with irregularly spaced data? }

The Figure was for regularly spaced data. We have repeated the experiment with irregularly space $x$-values (simulated from a uniform distribution on $[0,n]$). We get very similar timings as for the regularly spaced data (see new Figure 6).

\end{itemize}


\end{document}
\begin{itemize}
\item {\em The current package version is 1.0.3. Unfortunately the package does not contain a NEWS file which would allow to easily assess which functionality was added since version 0.0.3. Presumably the extensions cover unevenly spaced data, heterogeneous noise variance, and the possibility of a grid of potential change locations different from the locations of the data points.}

We have now added a NEWS file.

\item {\em The manuscript contains a section on R packages for changepoint detection. However, this section fails to relate the functionality provided by these packages to the functionality provided by the presented package. It would seem important to highlight for which use cases which packages would seem appropriate to guide potential users. In addition for the Journal of Statistical Software the software overview should cover statistical software packages in general, not only R packages.}

We have changed the title of this section. However, we are unaware of many software packages outside of R that perform changepoint detection, and none that perform change-in-slope. There is one package in Python, called ruptures, that has only been developed recently and claims to be the first Python package for changepoints. However this does not do change-in-slope. We have added a reference to this package within the paper. 

The paper emphasises the difference in the methods for change-in-slope detecting in the first part of the Introduction (see also Figure 1), and now states more clearly the extra functionality of the cpop package over the packages for NOT and trend-filtering.

\item {\em Function cpop has an argument sd with default value 1. It seems questionable that this is a reasonable default value for any kind of data but maybe rather a data-driven default value would seem more appropriate. To enhance the usability of the package more guidance should be provided on how this argument should be chosen and for example provide a function argument specification which determines a data-driven value. Note that the Journal of Statistical Software aims at publishing statistical software which particularly helps users in their data analysis providing suitable tools to do not conveniently.}

We have now implemented a default method for estimating the standard deviation of the noise -- based on the variance of the second differences of the data. We have also added a warning if the user does not specify the standard deviation (and thus this default is used). We have similarly added a warning if the user does not specify the penalty (beta) -- as errors in specifying sd or beta have the same affect of impacting the estimate of the number of changepoints.

How to estimate the noise standard deviation is covered in some of the examples int he paper, and in particular we suggest using the CROPS function to run CPOP with a range of beta values (which is equivalent to running it for a fixed beta value and a range of sd values).

\item {\em Loading the package gives:}
\begin{verbatim}
> library("cpop")
Loading required package: crops
Loading required package: pacman
Attaching package: ‘cpop’
The following object is masked from ‘package:stats’:
fitted
The following object is masked from ‘package:methods’:
show
\end{verbatim}
{\em Clearly the masking should be avoided. E.g., by including}
\begin{verbatim}
    setGeneric("fitted", package = "stats")
\end{verbatim}
{\em and also importing show from methods.}

The methods shown and fitted in the cpop package were being set as generic methods with no reference to any existing S4 method of that name. This has now been prevented by including a test to see if these methods have already been set as generic, for instance, by another package.


\item {\em The output of}
\begin{verbatim}
> help(package = "cpop")
\end{verbatim}
{\em indicates that the vignette name "article" might not be very suitable and the documentation}
\begin{verbatim}
cpop cpop
\end{verbatim}
{\em is not very informative. Also the titles are not consistent in title or sentence style.
The description contains (2018) but it seems the paper was published in 2019.}

The vignette has been renamed to cpop. The documentation for the cpop function has been extended and the titles
for all sections of the documentation has been changed to sentence style. The date has been corrected in the packages DESCRIPTION file.

\item {\em The code defines twice a function null2na which is not called within the package and also not exported.}

The redundant functions have been removed from the package.

\item {\em Running example(cpop) and then using estimate() gives the following results:}
\begin{verbatim}
> estimate(res.true, x = 100)
x y_hat
1 100 11.0609
> estimate(res.true, x = res.true@x[50])
x y_hat
100 11.0609
> tail(estimate(res.true))
x y_hat
45 81.00 10.37956
46 84.64 10.43842
47 88.36 10.49856
48 92.16 10.56001
49 96.04 10.62274
50 100.00 10.68677
\end{verbatim}
{\em It is unclear why results differ for specific locations in case more locations are simultaneously evaluated.}

This incorrect behaviour was a consequence of a coding error in the estimate funciton. This has now been corrected.

\item {\em The code shown in the manuscript is}
\begin{verbatim}
R> res <- cpop(y, x, grid, sd = sqrt(sig2), minseg = 0.2, 
beta = 2*log(200))    
\end{verbatim}
{\em with comment in the text:
To avoid the potential for adding multiple changepoints between two observations, we set a minimum segment length of 0.09 (as the largest distance between consecutive x values is 0.08).
The code in the replication material then reads:}
\begin{verbatim}
res <- cpop(y, x, grid, sd=sqrt(sig2), minseg = 0.09,
beta=2*log(200))
\end{verbatim}

The code in the manuscript has been changed 
so that the article and the reproduction script are now consistent.

\end{itemize}
\end{document}